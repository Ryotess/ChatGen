{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample_code, we will explain the detail of using this package to generate your own chat dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this instruction, we prepared a sample dataset for you. Let's first take a look of this sample dataset, you can open the sample_dataset.xlsx through your xlsx reader(ex. Excel, GoogleSheet).  \n",
    "You would see there are several sheet in this file: QuestionAskingMerge, Self_cognition, End_of_Conversation, PC and Screen\n",
    " \n",
    "This file represents the example raw data format for users, you can follow the rules to prepare your own input data:  \n",
    "1. Categorize your QA data into different sheets\n",
    "2. Annotate the QA data with:\n",
    "    - Type (category of question)\n",
    "    - Level (level of question)\n",
    "    - No (question number)\n",
    "    - UID (Made by Level_No)\n",
    "    - Parent (The parent QA UID)\n",
    "    - Well-formed question and Well-formed answers  \n",
    "\n",
    "    Type is the Category of this question  \n",
    "    Level means the hierarchical status of this question, for example, the initial QA of a conversation should be A level (ex. Q: Hi, I wanna ask you a question. A: Hi, What you wanna ask?),  \n",
    "    and the QA followed with A should be B level (ex. Q: Where is the capital of Taiwan? A: The capital of Taiwan is Taipei City), and so on.  \n",
    "    In this beta version, we only provide four levels: A, B ,C and Z, where Z represents the final QA of this conversation(ex. Q: Thank you! A: You're welcome!)  \n",
    "    You can follow the pattern of the sample file to get a more detail understand.\n",
    "\n",
    "3. Put all the sheet(all different type of categories) into a merged sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After prepared your own dataset, you can follow the code below to generate your own chat dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages we would use in this instruction, and set the path of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "from chatgen.chat_algo import ChatAlgo\n",
    "from chatgen.data_loader import load_xlsx, create_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data path\n",
    "input_file = \"./dataset/sample_dataset.xlsx\" # sample dataset or replace with your own dataset\n",
    "sheet_name = 'QuestionAskingMerge' # the sheet we would use or replace with your own merged sheet name\n",
    "output_file = \"./output/conversations.json\" # output path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can load the file by function ```create_input_data()``` (we provide ```load_csv()```for you to load the sample data, but you can also load your dataset by your own way into Pandas DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data & Create input data\n",
    "data = load_xlsx(input_file, sheet_name)\n",
    "input_data = create_input_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look of the input_data\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm, we consider some factors to generate the chat dataset by simulate the behavior of human.  \n",
    "\n",
    "First, you should consider the ```max_depth```, which is, how deep the conversation you wanna generate.   \n",
    "For example, If you wanna finetune a chatbot that are an expert for debate, mostly debate is a long-term conversation, in this case, you should set the max_depth larger.  \n",
    "\n",
    "Next, the ```init_weight``` is the probability of B, C levels, and ```final_level_weight``` is the Z level for initialization. Since mostly we start a conversation by A level, in default we set ```init_weight``` to be 0.05 and ```final_level_weight``` to be 10^-6, which generate good dataset based on our empiric.  \n",
    "\n",
    "During the time of generation, the algorithm will follow the below pattern, repeat till generate total number of ```generate_times``` conversations:  \n",
    "\n",
    "For each conversation in generated dataset:\n",
    "1. Randomly choose a number from ```1```~```max_depth``` as the number of rounds of the conversations under generating\n",
    "2. Initialize the sampling probability by ```init_weight```, ```final_level_weight``` and random choose the depth of this conversation from ```max_depth```\n",
    "3. Sample one observation from input data base on the probability\n",
    "4. Punish the probability of the observation we just sampled by ```current_punish```(since most of the time we don't repeat the same QA in real life)\n",
    "5. If the observation have childs, reward its childs. Otherwise, reward the same level of the observation by ```child_reward``` (This will boost the conversation continue)\n",
    "6. Modified the probability of levels by ```ORD``` matrix  \n",
    "You can check ```./chatgen/chat_algo.py``` to see that matrix, by each row we get the weights of each level(column) base on observation's level(row), the value is base on our research empiric, this also can help boost the conversation continue\n",
    "7. repeat 3~6. If the sample times reach ```final_weighting_threshold```, reward Z level by ```final_level_reward``` (people not always complete the conversation, there is a probability that conversation end before getting the answer they want)  \n",
    "\n",
    "That's it! You can change the parameter base on your dataset to get the result you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params setting\n",
    "params = {\n",
    "    'system_prompt':'You are a helpful artificial intelligent assistant, your name is ChatGenBot.',\n",
    "    'generate_times': 1000,\n",
    "    'max_depth': 6,\n",
    "    'init_weight': 0.05,\n",
    "    'final_level_weight':0.000001,\n",
    "    'current_punish': 0.01,\n",
    "    'child_reward': 8000,\n",
    "    'final_weighting_threshold': 2,\n",
    "    'final_level_reward': 5000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using ```create_chat_history()```, the it will generate chat history of input data with params you just set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation dataset\n",
    "chat_algo = ChatAlgo(input_data) # initialization\n",
    "chat_algo.create_chat_history(**params) # Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generation, you can user ```sample_output()``` to take a glimpse of generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_algo.sample_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can save the result to json file with ```to_json()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_algo.to_json(output_file) # save to JSON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA7885",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
