{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample_code, we will explain the detail of using this package to generate your own chat dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this instruction, we prepared a sample dataset for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's first take a look of this sample dataset, you can open the sample_dataset.xlsx through your xlsx reader(ex. Excel, GoogleSheet).  \n",
    "You would see there are several sheet in this file: QuestionAskingMerge, Self_cognition, End_of_Conversation, PC and Screen  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file represents the example raw data format for users, you can follow the rules to prepare your own input data:  \n",
    "1. Categorize your QA data into different sheets\n",
    "2. Annotate the QA data with:\n",
    "    - Type (category of question)\n",
    "    - Level (level of question)\n",
    "    - No (question number)\n",
    "    - UID (Made by Level_No)\n",
    "    - Parent (The parent QA UID)\n",
    "    - Well-formed question and Well-formed answers  \n",
    "\n",
    "    #Type is the Category of this question  \n",
    "    #Level means the hierarchical status of this question, for example, the initial QA of a conversation should be A level  \n",
    "    (ex. Q: Hi, I wanna ask you a question. A: Hi, What you wanna ask?),  \n",
    "    and the QA followed with A should be B level  \n",
    "    (ex. Q: Where is the capital of Taiwan? A: The capital of Taiwan is Taipei City), and so on.  \n",
    "    #In this beta version, we only provide four levels: A, B ,C and Z, where Z represents the final QA of this conversation(ex. Q: Thank you! A: You're welcome!)  \n",
    "    #You can follow the pattern of the sample file to get a more detail understand.\n",
    "\n",
    "3. Put all the sheet(all different type of categories) into a merged sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After prepared your own dataset, you can follow the code below to generate your own chat dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages we would use in this instruction, and set the path of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "from chatgen.chat_algo import ChatAlgo\n",
    "from chatgen.data_loader import load_xlsx, create_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data path\n",
    "input_file = \"./dataset/sample_dataset.xlsx\" # sample dataset or replace with your own dataset\n",
    "sheet_name = 'QuestionAskingMerge' # the sheet we would use or replace with your own merged sheet name\n",
    "output_file = \"./output/conversations.json\" # output path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can load the file by function ```create_input_data()``` (we provide ```load_csv()```for you to load the sample data, but you can also load your dataset by your own way into Pandas DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data & Create input data\n",
    "data = load_xlsx(input_file, sheet_name)\n",
    "input_data = create_input_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look of the input_data\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm, we consider some factors to generate the chat dataset by simulate the behavior of human.  \n",
    "First, you should consider the ```max_depth```, which is, how deep the conversation you wanna generate.   \n",
    "For example, If you wanna finetune a chatbot that are an expert for debate, mostly debate is a long-term conversation, in this case, you should set the max_depth larger.  \n",
    "And the ```init_weight``` is the probability of B, C levels and ```final_level_weight``` is the Z level for initialization, since mostly we start a conversation by A level.  \n",
    "\n",
    "During each time of the generation, the algorithm will follow the below pattern, repeat till generate ```generate_times``` conversations:  \n",
    "\n",
    "1. Random choice a number from ```1```~```max_depth``` as the number of rounds of this conversations\n",
    "2. Initialize the probability and randomize the depth of this from\n",
    "3. sample one observation from input data\n",
    "4. punish the probability of the observation (since most of the time we don't repeat the same QA in real life)\n",
    "5. if the observation have childs, reward its childs. Otherwise, reward the same level of observation by ```child_reward``` (This will boost the conversation continue)\n",
    "6. modified the probability of levels by ORD matrix  \n",
    "\n",
    "you can check ```./chatgen/chat_algo.py``` to see that matrix, by each row we get the weights of each level(column) base on observation's level(row), the value is base on our research empiric, this also can help boost the conversation continue\n",
    "\n",
    "7. repeat 3~6. If the sample times reach ```final_weighting_threshold```, reward Z level by ```final_level_reward``` (people not always complete the conversation, there is a probability that conversation end before getting answer)  \n",
    "\n",
    "You can change the parameter base on your dataset to get the result you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params setting\n",
    "params = {\n",
    "    'system_prompt':'You are a helpful artificial intelligent assistant, your name is ChatGenBot.',\n",
    "    'generate_times': 1000,\n",
    "    'max_depth': 6,\n",
    "    'init_weight': 0.05,\n",
    "    'final_level_weight':0.000001,\n",
    "    'current_punish': 0.01,\n",
    "    'child_reward': 8000,\n",
    "    'final_weighting_threshold': 2,\n",
    "    'final_level_reward': 5000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using ```create_chat_history()```, the it will generate chat history of input data with params you just set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation dataset\n",
    "chat_algo = ChatAlgo(input_data) # initialization\n",
    "chat_algo.create_chat_history(**params) # Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generation, you can user ```sample_output()``` to take a glimpse of generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_algo.sample_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can save the result to json file with ```to_json()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_algo.to_json(output_file) # save to JSON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA7885",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
